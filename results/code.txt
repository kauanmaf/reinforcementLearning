# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# Load the dataset
url = 'https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv'
df = pd.read_csv(url)

# Clean the data
print("Shape of the dataset: ", df.shape)
print("Dataset's summary statistics: \n", df.describe())
print("Missing values in dataset: \n", df.isnull().sum())

# Remove missing values from the dataset
df.dropna(inplace=True)
print("Shape of the dataset after removing missing values: ", df.shape)

# Handle outliers
Q1 = df['RM'].quantile(0.25)
Q3 = df['RM'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df = df[(df['RM'] > lower_bound) & (df['RM'] < upper_bound)]
print("Shape of the dataset after removing outliers: ", df.shape)

# Transform the data
print("Transformation types: \n", df.info())

# Label Encoding for categorical variables
categorical_vars = ['RM', 'LSTAT']
encoder = LabelEncoder()
for col in categorical_vars:
    df[col] = encoder.fit_transform(df[col])

# One-hot encoding for categorical variables
one_hot_cols = ['CHAS', 'NOX', 'PTRATIO', 'B', 'DIS', 'RAD', 'TAX', 'ZN', 'TINDUS']
encoder = OneHotEncoder(handle_unknown='ignore')
transformer = ColumnTransformer([('encoder', encoder, one_hot_cols)], remainder='passthrough')
transformer.fit(df)
X_train = transformer.transform(df)

# Scaling of numerical variables
scaler = StandardScaler()
X_train_nums = scaler.fit_transform(X_train[:, :-9])

# Prepare the data for analysis
X = X_train_nums[:, :6]
y = df['MEDV']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model on the dataset
regr = LinearRegression()
regr.fit(X_train, y_train)

# Measure the performance of the linear regression model
y_pred = regr.predict(X_test)
print('Mean Squared Error: ', mean_squared_error(y_test, y_pred))
print('Cross-validation score: ', cross_val_score(regr, X, y, cv=5, n_jobs=-1).mean())

# Perform grid search on the linear regression model
param_grid = {'fit_intercept': [True, False], 'copy_X': [True, False]}
grid_search = GridSearchCV(regr, param_grid, cv=5, n_jobs=-1)
grid_search.fit(X_train, y_train)
print('Best Parameters: ', grid_search.best_params_)
print('Best Score: ', grid_search.best_score_)

# Decision Tree Regressor Model
dt_regr = DecisionTreeRegressor(random_state=42)
dt_regr.fit(X_train, y_train)
y_pred_dt = dt_regr.predict(X_test)
print('Decision Tree Regression Mean Squared Error: ', mean_squared_error(y_test, y_pred_dt))
print('Decision Tree Regression Cross-validation score: ', cross_val_score(dt_regr, X, y, cv=5, n_jobs=-1).mean())

# Random Forest Regressor Model
rf_regr = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regr.fit(X_train, y_train)
y_pred_rf = rf_regr.predict(X_test)
print('Random Forest Regression Mean Squared Error: ', mean_squared_error(y_test, y_pred_rf))
print('Random Forest Regression Cross-validation score: ', cross_val_score(rf_regr, X, y, cv=5, n_jobs=-1).mean())

# Support Vector Regressor Model
svr_regr = SVR(kernel='rbf', C=1e3, gamma=0.1)
svr_regr.fit(X_train, y_train)
y_pred_svr = svr_regr.predict(X_test)
print('Support Vector Regression Mean Squared Error: ', mean_squared_error(y_test, y_pred_svr))
print('Support Vector Regression Cross-validation score: ', cross_val_score(svr_regr, X, y, cv=5, n_jobs=-1).mean())

# Gradient Boosting Regressor Model
gb_regr = GradientBoostingRegressor()
gb_regr.fit(X_train, y_train)
y_pred_gb = gb_regr.predict(X_test)
print('Gradient Boosting Regression Mean Squared Error: ', mean_squared_error(y_test, y_pred_gb))
print('Gradient Boosting Regression Cross-validation score: ', cross_val_score(gb_regr, X, y, cv=5, n_jobs=-1).mean())

# Train and validate the Decision Tree Regressor model
train_sizes, train_scores, test_scores = learning_curve(dt_regr, X, y, cv=5, n_jobs=-1, train_sizes=np.logspace(-5, 0, 20, dtype=int))
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, np.mean(np.array(train_scores), axis=0), marker='o', drawstyle='steps-post', label='Training Score')
plt.plot(train_sizes, np.mean(np.array(test_scores), axis=0), marker='o', drawstyle='steps-post', label='Cross-validation Score')
plt.title('Training and Testing Errors in Relation to Training Size')
plt.xlabel('Training Size')
plt.ylabel('Error')
plt.legend()
plt.show()

# Train and validate the Random Forest Regressor model
train_sizes, train_scores, test_scores = learning_curve(rf_regr, X, y, cv=5, n_jobs=-1, train_sizes=np.logspace(-5, 0, 20, dtype=int))
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, np.mean(np.array(train_scores), axis=0), marker='o', drawstyle='steps-post', label='Training Score')
plt.plot(train_sizes, np.mean(np.array(test_scores), axis=0), marker='o', drawstyle='steps-post', label='Cross-validation Score')
plt.title('Training and Testing Errors in Relation to Training Size')
plt.xlabel('Training Size')
plt.ylabel('Error')
plt.legend()
plt.show()

# Train and validate the Support Vector Regressor model
train_sizes, train_scores, test_scores = learning_curve(svr_regr, X, y, cv=5, n_jobs=-1, train_sizes=np.logspace(-5, 0, 20, dtype=int))
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, np.mean(np.array(train_scores), axis=0), marker='o', drawstyle='steps-post', label='Training Score')
plt.plot(train_sizes, np.mean(np.array(test_scores), axis=0), marker='o', drawstyle='steps-post', label='Cross-validation Score')
plt.title('Training and Testing Errors in Relation to Training Size')
plt.xlabel('Training Size')
plt.ylabel('Error')
plt.legend()
plt.show()

# Train and validate the Gradient Boosting Regressor model
train_sizes, train_scores, test_scores = learning_curve(gb_regr, X, y, cv=5, n_jobs=-1, train_sizes=np.logspace(-5, 0, 20, dtype=int))
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, np.mean(np.array(train_scores), axis=0), marker='o', drawstyle='steps-post', label='Training Score')
plt.plot(train_sizes, np.mean(np.array(test_scores), axis=0), marker='o', drawstyle='steps-post', label='Cross-validation Score')
plt.title('Training and Testing Errors in Relation to Training Size')
plt.xlabel('Training Size')
plt.ylabel('Error')
plt.legend()
plt.show()